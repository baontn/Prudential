# -*- coding: utf-8 -*-
"""Pru1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KEjwTp1DbU5iPnQuM_C3FgzTxpZim9cl
"""

!git clone https://github.com/baontn/Prudential.git

!ls Prudential

"""#Import and Functions"""

import pandas as pd
import numpy as np
import sklearn
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
!pip install researchpy
import researchpy
from keras.wrappers.scikit_learn import KerasClassifier

from numpy import where
from numpy import meshgrid
from numpy import arange
from numpy import hstack
from numpy import std
from numpy import mean


from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

from sklearn.datasets import make_classification
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate

sns.set()

# Evaluation using cross validation
def eval_cv(classifier, metric):
    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)
    # prepare the cross-validation procedure
    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
    # evaluate model
    scores = cross_val_score(classifier, X, y, scoring=metric, cv=cv, n_jobs=-1)
    # report performance
    return mean(scores)

def calculate_metrics(df, method):
  '''
  Calculate the metrics
  '''
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, annot_kws={"size": 16})
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred)
    df.loc[method] = [accuracy, precision, recall, f1, auc]

def calculate_cv(df, classifier, method):
  '''
  Do the the cross validation of all metrics
  '''
    accuracy = eval_cv(classifier, 'accuracy')
    precision = eval_cv(classifier, 'precision')
    recall = eval_cv(classifier, 'recall')
    f1 = eval_cv(classifier, 'f1')
    auc = eval_cv(classifier, 'roc_auc')
    df.loc[method] = [accuracy, precision, recall, f1, auc]

def cramer_v(dataframe, y_column, column_list=None,):
  '''
  Function used to calculate the cramer's v correlation between variables
  '''
  if column_list is not None:
    for a in column_list:
      print(a)
      myfield1 = dataframe[a]
      myfield2 = y_column
      contTable = pd.crosstab(myfield1, myfield2)
      crosstab, results = researchpy.crosstab(myfield1, myfield2, test='chi-square')
      results
      print(results)
      df= (contTable.shape[0]-1)*(contTable.shape[1]-1)
      print("degree of freedom: ", df)
  else:
    for a in dataframe.columns:
      print(a)
      myfield1 = dataframe[a]
      myfield2 = y_column
      contTable = pd.crosstab(myfield1, myfield2)
      crosstab, results = researchpy.crosstab(myfield1, myfield2, test='chi-square')
      results
      print(results)
      df= (contTable.shape[0]-1)*(contTable.shape[1]-1)
      print("degree of freedom: ", df)

def get_xy(df, nominal_list, drop_list=None, test=None):
  """
  dataframe: dataframe to obtain the x and y
  nominal_list: list of columns with nominal values that needed to be encoded
  drop_list: list of columns to drop
  test: if test = True means the testdataset is being used, hence no encode for y
  """
  df1 = df.copy()
  if drop_list is not None: 
    df1.drop(drop_list, axis=1, inplace=True)

  x = df1.iloc[:,:-1]
  y = df1.iloc[:,-1]
  encode = LabelEncoder()
  y = encode.fit_transform(y)
  #Filling NA values
  imputer = SimpleImputer(missing_values = np.NaN, strategy='most_frequent')
  impute = imputer.fit(x.iloc[:, :])
  x.iloc[:,:] = imputer.transform(x.iloc[:,:])
  for i in nominal_list:
    encode2 = LabelEncoder()
    x.iloc[:,i] = encode2.fit_transform(x.iloc[:,i])
  if test is None:
    return x,y, encode
  if test is not None:
    return x, y

raw_data = pd.read_excel('Prudential/club_churn_train.xlsx')

raw_data

listt = [2,3,6,9] #list of nominal variables, after drop columns

df1 = raw_data.copy()

df1

x, y, encode1 = get_xy(df1, listt, ['ID', 'MEMBERSHIP_NUMBER', 'AGENT_CODE', 'START_DATE', 'END_DATE'])

my_test_size = 0.25
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=my_test_size, shuffle=False)
X_train

#Feature standardizing of X
#Using Standard scaler
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

X_train.shape

#Get the original X_test
X_test_orig = sc_X.inverse_transform(X_test)
display(pd.DataFrame(X_test_orig))

# included methods/models and metrics
methods = ['Logistic Regression', 'Kernel SVM', 'Random Forest', 'ANN']
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 score', 'ROC AUC']

# create model quality dataframe
quality_df = pd.DataFrame(columns = metrics, index = methods)
display(quality_df)

# create model quality cross validation dataframe
quality_cv_df = pd.DataFrame(columns = metrics, index = methods)
display(quality_cv_df)

"""# Logistics Regression"""

# Fit the model
from sklearn.linear_model import LogisticRegression
classifierLR = LogisticRegression(random_state = 0)
classifierLR.fit(X_train, y_train)

#Predicting test set result
y_pred = classifierLR.predict(X_test)
display(pd.DataFrame(y_pred))

# Evaluate the model
method = 'Logistic Regression'
calculate_metrics(quality_df, method)
display(quality_df)
calculate_cv(quality_cv_df, classifierLR, method)
display(quality_cv_df)

"""# Kernel SVM"""

# Fit the model
from sklearn.svm import SVC
classifierSVM = SVC(kernel = 'rbf',  random_state = 0)
classifierSVM.fit(X_train, y_train)

#Predict the test set 
y_pred = classifierSVM.predict(X_test)
display(pd.DataFrame(y_pred))

# Evaluate the model
method = 'Kernel SVM'
calculate_metrics(quality_df, method)
display(quality_df)
calculate_cv(quality_cv_df, classifierSVM, method)
display(quality_cv_df)

"""# ANN"""

#Make an ANN
from keras.models import Sequential
from keras.layers import Dense

train_dim = X_train.shape[1]

#Initialie ANN
classifier = Sequential()

#Adding input layer and hidden layers, init = initialization of weight
classifier.add(Dense(128, activation = 'relu', input_dim = train_dim))
classifier.add(Dense(256))
classifier.add(Dense(256))
#Adding output layer
classifier.add(Dense(1, activation = 'sigmoid'))

#compile ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

#Fitting ANN to training set
classifier.fit(X_train, y_train, batch_size = 24,validation_data=(X_test, y_test), epochs = 20)

y_hat= classifier.predict(X_test)
y_pred = (y_hat > 0.5)
display(pd.DataFrame(y_pred))

def build_classifier():
	classifier = Sequential()
	
	classifier.add(Dense(
		units = 128,
		kernel_initializer="uniform",
		activation="relu",
		input_dim = 10
		))
	
	classifier.add(Dense(
		units = 256,
		kernel_initializer="uniform",
		activation="relu"
		))
	
	classifier.add(Dense(
		units = 1,
		kernel_initializer="uniform",
		activation="sigmoid"
		))
	
	classifier.compile(
		optimizer = "adam",
		loss="binary_crossentropy",
		metrics=['accuracy']
		)
	
	return classifier

# Evaluate the model
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, annot_kws={"size": 16})
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)
quality_df.loc['ANN'] = [accuracy, precision, recall, f1, auc]
display(quality_df)


classifier = KerasClassifier(build_fn = build_classifier, 
                             batch_size = 24, 
                             nb_epoch = 50)
method=['ANN']
scores =  cross_validate(estimator=classifier, scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],
		X=X_train,
		y=y_train,
		cv=10)


accuracy = mean(scores['test_accuracy'])
precision = mean(scores['test_precision'])
recall = mean(scores['test_recall'])
f1 = mean(scores['test_f1'])
auc = mean(scores['test_roc_auc'])

for score in scores.keys():
  score= mean(scores[score])
quality_cv_df.loc[method] = [accuracy, precision, recall, f1, auc]
display(quality_cv_df)

"""# Random Forest


"""

# Fit the model
from sklearn.ensemble import RandomForestClassifier
classifierRF = RandomForestClassifier(n_estimators = 100, oob_score=True, criterion = 'entropy', random_state = 0)
classifierRF.fit(X_train, y_train)

#Predict the test set 
y_pred = classifierRF.predict(X_test)
display(pd.DataFrame(y_pred))

# Evaluate the model
method = 'Random Forest'
calculate_metrics(quality_df, method)
display(quality_df)
calculate_cv(quality_cv_df, classifierRF, method)
display(quality_cv_df)

"""# Predict - Test Dataset"""

test_file = pd.read_excel('Prudential/club_churn_test.xlsx')

test_file

def pred_test(df, model,encode,listt, drop_list, name="club_churn_test_sol"):
  '''
  df: dataframe to predict
  model: model use to predict
  encode: to inverse transform y values
  name: name of the output file
  '''
  x, y = get_xy(test_file, listt, drop_list, test="Not none")
  #Feature standardizing of X
  from sklearn.preprocessing import StandardScaler
  sc_X = StandardScaler()
  x = sc_X.fit_transform(x)
  y_pred = model.predict(x)
  y_pred = encode.inverse_transform(y_pred)
  df['MEMBERSHIP_STATUS'] = y_pred
  df.to_csv(name, index=False)

drop_list = ['ID', 'MEMBERSHIP_NUMBER', 'AGENT_CODE', 'START_DATE']

listt = [2,3,6,9]

pred_test(test_file, classifierRF, encode1, listt,drop_list )

x, y = get_xy(test_file, listt, ['ID', 'MEMBERSHIP_NUMBER', 'AGENT_CODE', 'START_DATE'], test="Not none")

#Feature standardizing of X
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
x = sc_X.fit_transform(x)

y_pred = classifierRF.predict(x)

y_pred = encode1.inverse_transform(y_pred)

test_file['MEMBERSHIP_STATUS'] = y_pred

test_file

